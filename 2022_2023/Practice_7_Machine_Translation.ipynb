{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoLaQuatra/DeepNLP/blob/main/2022_2023/Practice_6_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOEr39Bst_5"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 6:** Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9F11ek-0L8g"
      },
      "source": [
        "## **Machine Translation**\n",
        "\n",
        "Machine Translation is a sub-field of Natural Language Processing that aims at translating a text from a source language to a target language. In this practice, we will experiment with a Transformer-based model for Machine Translation. Specifically, we will benchmark the performance of a pre-trained MT model on Italian-English and English-Italian translation tasks.\n",
        "\n",
        "![](https://www.deepl.com/img/press/desktop_ENIT_2020-01.png)\n",
        "\n",
        "In this practice we will use a data collection provided by [tatoeba](https://tatoeba.org/). The following cell download a subset of the data collection, containing parallel Italian-English sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41mtAy092sCC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/train_it_en.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/test_it_en.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCTUyoq2rZo"
      },
      "source": [
        "### **Question 1: parsing data**\n",
        "\n",
        "The first step is to parse the data collection to generate a list of sentence pairs. The data are provided in `tsv` format, where each line contains a sentence pair in the following format:\n",
        "\n",
        "`<source_language_sentence>\\t<target_language_sentence>\\n`\n",
        "\n",
        "You are provided with a training and a test set. For this question you should parse both data splits and store them in your preferred data structure.\n",
        "\n",
        "**Note:** store train and test set into separate data objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rCk2Jcm4eHF"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyW-AESi3yYe"
      },
      "source": [
        "### **Question 2: pretrained MT models**\n",
        "\n",
        "Pre-trained MT models are released to the public to allow researchers to experiment with them. In this question you will load a pre-trained MT model and use it to translate sentences from Italian to English and vice-versa.\n",
        "\n",
        "[EasyNMT](https://github.com/UKPLab/EasyNMT) is a Python library that provides an easy-to-use interface to pre-trained MT models. It provides a simple wrapper over HuggingFace transformers library for machine translation. In this question you will use EasyNMT to load a pre-trained MT model and translate sentences from Italian to English and vice-versa:\n",
        "\n",
        "- Load the pre-trained model for a specific direction (e.g., Italian-English or English-Italian)\n",
        "- Translate all the sentences in the test set from the source language to the target language.\n",
        "\n",
        "\n",
        "**Note**: the choice for the MT model is up to you.\n",
        "**Note 2**: store the translated sentences in both directions using the data structure of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA-f6Huz4bWF"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv-4koeH6yvt"
      },
      "source": [
        "### **Question 3: BLEU scores**\n",
        "\n",
        "One of the most common metrics used to evaluate MT models is the BLEU score. The BLEU score is a metric that compares the predicted translation with the reference translation. The higher the BLEU score, the better the translation. In this question you will compute the BLEU score for the translated sentences.\n",
        "\n",
        "Evaluate the selected MT model using [BLEU evaluation metric](https://github.com/mjpost/sacrebleu). Report scores for both translation directions (`EN->IT`, `IT->EN`). \n",
        "\n",
        "The following cell install the `saclrebleu` library that can be used to compute the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9dLMLRw7SDu"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BBY0_1X7WAr"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRoH_9ox03ME"
      },
      "source": [
        "### **Question 4: finetuning Seq2Seq model (IT->EN)**\n",
        "\n",
        "One of the most common paradigm for modern transformer architectures is to use pre-training to learn a generic representation of the language and then fine-tune the model on a specific task. However, you can also fine-tune a model that is already pre-trained on a specific task to another dataset. In this question you will fine-tune a pre-trained MT model on a tatoeba dataset.\n",
        "\n",
        "Exploit the [Trainer API](https://huggingface.co/transformers/training.html#fine-tuning-in-pytorch-with-the-trainer-api) to finetune and evaluate a [MarianMT](https://arxiv.org/pdf/1804.00344.pdf) sequence to sequence model for machine translation. The documentation for MarianMT is available [here](https://huggingface.co/transformers/model_doc/marian.html).\n",
        "\n",
        "You can use the code from the previous practices as a starting point (seq2seq model for abstractive summarization).\n",
        "\n",
        "**Note 1:** select the pre-trained model according to the input-output pair (it-en)\n",
        "\n",
        "**Note 2:** for the lab practice, please use a sub-set of the training data (e.g., 10000 sentences for training and 1000 sentences for validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0V98Y1uTrzR"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwS-7GrlSrxF"
      },
      "source": [
        "### **Question 5: Model evaluation**\n",
        "\n",
        "Using the test set provided at the beginning of the practice, evaluate the fine-tuned model and report the BLEU score. How does the BLEU score compare with the one obtained in the previous question?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL0kUCsVS7b6"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOEi+akM7vvByJwE//tGDgv",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Practice 6 - Machine Translation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
