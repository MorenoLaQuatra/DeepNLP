{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/Practice_4_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entities Recognition & Intent Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQde2M-U4KV"
      },
      "source": [
        "## Named Entities Recognition (NER)\n",
        "\n",
        "The Named Entity Recognition task aims at identifying and classifying named entities in a text. Named entities are real-world objects such as persons, locations, organizations, etc. The task takes as input a sentence and determines the boundaries of the named entities and their type. \n",
        "\n",
        "For example, given the sentence:\n",
        "\n",
        "```\n",
        "I went to Paris last week.\n",
        "```\n",
        "\n",
        "the task is to identify the named entity `Paris` as a location.\n",
        "\n",
        "Hereafter an illustration of the NER task:\n",
        "\n",
        "![https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg](https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VXjktGnWUTm"
      },
      "source": [
        "In the first part of this practice, you will:\n",
        "- explore the NER task using pre-trained models available on Spacy and HuggingFace\n",
        "- evaluate the performance of a SpaCy NER model on a custom dataset\n",
        "- evaluate the performance of a HuggingFace NER model on a custom dataset\n",
        "\n",
        "NB: the metric used to evaluate the performance of the models is seqeval, which is a library for evaluating sequence labeling tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9elJruvpWAQ_"
      },
      "source": [
        "### **Question 1: data preparation**\n",
        "\n",
        "The first step is to prepare the data. In this practice, you will use the WikiGold dataset[1][2], which is a collection of annotated sentences from Wikipedia. The dataset is available in [CONLL](https://simpletransformers.ai/docs/ner-data-formats/#text-file-in-conll-format) format. The dataset is available [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt). \n",
        "\n",
        "**Please, read carefully the following instructions before starting to work on the practice.**\n",
        "\n",
        "You need to extract clean sentences (no annotation) and, for each sentence, the corresponding annotations. The dataset has the following format:\n",
        "\n",
        "- `sentences`: list of sentences\n",
        "- `annotations`: list of list of entities (both string and class information). E.g., `[[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')], [('Osc-Dis', 'MISC'), ('Introduction 010', 'MISC'), ('Come', 'MISC')], ...]`. You can remove I- prefix because the data collection does not actually cotains valuable prefixes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[1] Balasuriya, Dominic, et al. \"Named entity recognition in wikipedia.\"\n",
        "    Proceedings of the 2009 Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources. Association for Computational Linguistics, 2009.\n",
        "\n",
        "[2] Nothman, Joel, et al. \"Learning multilingual named entity recognition\n",
        "    from Wikipedia.\" Artificial Intelligence 194 (2013): 151-175 \n",
        "\n",
        "---\n",
        "\n",
        "The following cell downloads the dataset on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeGD-RtlV_0k"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid spending too much time on data processing, the following cell prepare the dataset for you.\n",
        "After running the cell, you will have the following variables:\n",
        "- `sentences_with_labels`: a list of tokens with their corresponding labels\n",
        "- `sentences`: a list of the sentences in the dataset\n",
        "- `labels`: a list of lists of labels. Each element in the outer list corresponds to a list of labels for a sentence in the dataset.\n",
        "\n",
        "Hereafter an example of the provided data:\n",
        "\n",
        "```python\n",
        "sentences_with_labels[0] = [\n",
        "    ['010', 'I-MISC'], \n",
        "    ['is', 'O'], \n",
        "    ['the', 'O'], \n",
        "    ['tenth', 'O'], \n",
        "    ['album', 'O'], \n",
        "    ['from', 'O'], \n",
        "    ['Japanese', 'I-MISC'], \n",
        "    ['Punk', 'O'], \n",
        "    ['Techno', 'O'], \n",
        "    ['band', 'O'], \n",
        "    ['The', 'I-ORG'], \n",
        "    ['Mad', 'I-ORG'], \n",
        "    ['Capsule', 'I-ORG'], \n",
        "    ['Markets', 'I-ORG'], \n",
        "    ['.', 'O']\n",
        "]\n",
        "\n",
        "sentences[0] = [\n",
        "    '010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets .'\n",
        "]\n",
        "\n",
        "labels[0] = [\n",
        "    ('010', 'MISC'), \n",
        "    ('Japanese', 'MISC'), \n",
        "    ('The Mad Capsule Markets', 'ORG')\n",
        "]\n",
        "```\n",
        "\n",
        "Please, note that the labels are not in IOB format. You can ignore the I- prefix because the data collection does not actually contains valuable prefixes.\n",
        "Get familar with the data by printing the first 10 sentences and their corresponding labels. Which are the labels in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install datasets\n",
        "! pip install transformers\n",
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK19r9la4jUd"
      },
      "outputs": [],
      "source": [
        "def split_text_label(filename):\n",
        "    f = open(filename)\n",
        "    split_labeled_text = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "             if len(sentence) > 0:\n",
        "                 split_labeled_text.append(sentence)\n",
        "                 sentence = []\n",
        "             continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "    if len(sentence) > 0:\n",
        "        split_labeled_text.append(sentence)\n",
        "        sentence = []\n",
        "    return split_labeled_text\n",
        "sentences_with_labels = split_text_label(\"wikigold.conll.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (sentences_with_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sentences = [] \n",
        "\n",
        "for sent_list in sentences_with_labels:\n",
        "    sentence = [s[0] for s in sent_list] \n",
        "    sentence = \" \".join(sentence)\n",
        "    sentences.append(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = []\n",
        "overall_labels = [] \n",
        "for sent_list in sentences_with_labels:\n",
        "    current_labels = []\n",
        "    prev = \"O\"\n",
        "    current_entity = \"\"\n",
        "    for w, l in sent_list:\n",
        "        overall_labels.append(l)\n",
        "        if l != \"O\" and prev != \"O\":\n",
        "            if l == prev:\n",
        "                # continue entity\n",
        "                current_entity += w + \" \"\n",
        "            else:\n",
        "                # end prev and start a new one\n",
        "                current_labels.append((current_entity.strip(), prev.split(\"-\")[1]))\n",
        "                current_entity = w + \" \"\n",
        "        elif l == \"O\" and  prev != \"O\": \n",
        "            # end prev\n",
        "            current_labels.append((current_entity.strip(), prev.split(\"-\")[1]))\n",
        "            current_entity = \"\"\n",
        "        elif l != \"O\" and prev == \"O\":\n",
        "            # start new\n",
        "            current_entity = w + \" \"\n",
        "\n",
        "        prev = l\n",
        "    labels.append(current_labels)\n",
        "\n",
        "print (labels)\n",
        "overall_labels = list(set(overall_labels))\n",
        "overall_labels = [o for o in overall_labels if o != \"O\"] \n",
        "overall_labels = [o.split(\"-\")[1] for o in overall_labels] \n",
        "print (overall_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmIad5_ZUg6X"
      },
      "outputs": [],
      "source": [
        "# Your code and answers here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3VJcC9Bvnf"
      },
      "source": [
        "### **Question 2: inference with spacy for entity recognition**\n",
        "\n",
        "SpaCy is a free, open-source library for advanced Natural Language Processing in Python. It features NER models for different languages including English.\n",
        "The models are available [here](https://spacy.io/models). \n",
        "\n",
        "For this question you asked to instantiate a spacy model for English and perform inference on the sentences in the dataset. The English model contains a superset of the labels in the dataset. For this reason, you need to map the labels that are not in the dataset to the `MISC` label.\n",
        "\n",
        "You are expected to generate an output similar to the following:\n",
        "```python\n",
        "[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')]\n",
        "```\n",
        "\n",
        "Please pay attention to the token attributes (you can find more information [here](https://spacy.io/api/token#attributes)) and the entity attributes (you can find more information [here](https://spacy.io/api/entityrecognizer)).\n",
        "\n",
        "The following cell instantiates a spacy model for English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXMAjNvaIoj5"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf0NzZzjPheL"
      },
      "source": [
        "### **Question 3: compute metrics for evaluating NER**\n",
        "\n",
        "The output of NER models consists of a set of named entities. To evaluate the performance of a model, we need to compare the predicted named entities with the ground truth.\n",
        "\n",
        "For this question, you need to use [`eval4ner`](https://github.com/cyk1337/eval4ner) package to evaluate the performance of the model. \n",
        "\n",
        "**Note**: please use `pip install git+https://github.com/MorenoLaQuatra/eval4ner` to use a fixed version of the library. Before passing the parameter to the evaluation function, create a deepcopy of each variable:\n",
        "```python\n",
        "from copy import deepcopy\n",
        "sentences_copy = deepcopy(sentences) #pass sentences_copy to the eval script\n",
        "```\n",
        "\n",
        "The issue has been already reported to the original author."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACNK6UU98VZr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install git+https://github.com/MorenoLaQuatra/eval4ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlJM3PVa8SGS"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC19UMUZdJxc"
      },
      "source": [
        "### **Question 4: inference with transformers pipeline**\n",
        "\n",
        "Transformer-based models can be fine-tuned for token-level classification. The task is to classify each token in a sentence and assign it to a class.\n",
        "The NER task is a token-level classification task and the models can be used for performing inference on the sentences in the dataset.\n",
        "\n",
        "You can use the pipeline available on the HuggingFace [transformers library](https://huggingface.co/docs/transformers/main_classes/pipelines). The pipeline allows to perform inference on a list of sentences.\n",
        "\n",
        "Evaluate the **standard** model using the pipeline (`pipe = pipeline(\"ner\")`). Check the documentation here: https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline\n",
        "\n",
        "\n",
        "A few notes about the question (**read carefully**):\n",
        "1. The output of the pipeline differs with respect to spacy. Please be sure to process data correctly before running evaluation.\n",
        "2. `ignore_labels` parameter could be used to exclude labels from the prediction.\n",
        "3. `##` symbol is used when a token is a continuation of a previous one (Poli + ##TO). You may need to check this specific case to merge the tokens correctly.\n",
        "4. Use seqeval to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6pQuOdTZZVo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8I-9jTBI_5F"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrJbaVgi_ja"
      },
      "source": [
        "## Intent Detection\n",
        "\n",
        "In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of his/her behavior in interaction with a computer system, such as in search engines. Intent Detection is the identification and categorization of what a user online intended or wanted to find when they type or speak with a conversational agent (or a search engine).\n",
        "\n",
        "![https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png](https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png)\n",
        "\n",
        "In this section, you will use the [ATIS dataset](https://github.com/yvchen/JointSLU): https://github.com/yvchen/JointSLU ; https://www.kaggle.com/siddhadev/atis-dataset-clean/home\n",
        "\n",
        "The task is to classify the intent of a sentence. The dataset is split into train, validation and test sets. **Use the provided splits** to train and evaluate your models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L6-2ABir0yS"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nJxxjTj8Q9"
      },
      "source": [
        "### **Question 5: two-step classification model**\n",
        "\n",
        "Train a classification model to identify the intent from a given sentence. The model is required to leverage on pretrained BERT model to generate sentence embeddings (important: **no fine-tuning**). The model is required to use the embeddings to perform classification.\n",
        "\n",
        "Once extracted the embeddings, you can use any classifier you want. For example, you can use a linear classifier (e.g. Logistic Regression) or a neural network (e.g. MLP). For convenience, you can use the `sklearn` library for training the classifier (https://scikit-learn.org/stable/supervised_learning.html).\n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true)\n",
        "\n",
        "\n",
        "Assess the performance of the trained model (the model on top of BERT) on the test set by using the **classification accuracy**, **precision**, **recall** and **F1-score**. You can use the `sklearn` library for computing the metrics (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
        "\n",
        "\n",
        "Note: you can use the `sentence-transformers` library to generate sentence embeddings (https://www.sbert.net/docs/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STJSjSovq46n"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xrNtQTq5WW"
      },
      "source": [
        "### **Question 6: finetuning end-to-end classification model**\n",
        "\n",
        "Another approach is to fine-tune the BERT model for the classification task. A classification head is added on top of the pretrained BERT model. The classification head is trained end-to-end with the BERT model.\n",
        "This approach is more effective than the previous one because the model is trained end-to-end. However, the model requires more training time and resources.\n",
        "\n",
        "Train a new BERT model for the task of [sequence classification](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification) (include BERT fine-tuning).  \n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true)\n",
        "\n",
        "Assess the performance of the generated model by using the same metrics used in the previous question.\n",
        "\n",
        "Which model has better performance? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqlKATw5sJAY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNnykot1/7kDiSBh2rCcgxR",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Practice 4 - NER and Intent Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a88d8d275dc8276b143b02757b297c7b7ccc4199bd118b2f9ce33906ca7c97c0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
