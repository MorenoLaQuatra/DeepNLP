{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoLaQuatra/DeepNLP/blob/main/2022_2023/Practice_3_IR_and_Recommendation_systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o7x8zVR9gSK"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Information Retrieval & Elastic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIamxNpr_E99"
      },
      "source": [
        "### Download and setup ElasticSearch on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SziAua4o9UIl"
      },
      "outputs": [],
      "source": [
        "# Download and extract elasticsearch\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.10.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN1ll47sQIq-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# If issues are encountered with this section, ES can be manually started as follows:\n",
        "# ./elasticsearch-7.10.1/bin/elasticsearch\n",
        "\n",
        "# Start and wait for server\n",
        "server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "!sleep 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAAzxrUQFg_g"
      },
      "outputs": [],
      "source": [
        "# wait a bit then test\n",
        "!curl -X GET \"localhost:9200/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bb-wW4sZuw"
      },
      "source": [
        "## Information Retrieval\n",
        "\n",
        "Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of **texts**, images or sounds. (source: Wikipedia).\n",
        "\n",
        "This practice is intended for the creation of a wikipedia-based search engine. For the purpose of the practice, only a subset of the wikipedia pages will be used.\n",
        "\n",
        "Data Source: https://snap.stanford.edu/data/wikispeedia.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XxgpAo-VOg"
      },
      "source": [
        "Most of the information retrieval systems are based on the **PageRank** algorithm. It is a graph-based algorithm that assigns a score to each node of a graph, based on the number of incoming and outgoing links. The main steps of the algorithm are:\n",
        "\n",
        "1.   Assign a score of 1/N to each node, where N is the number of nodes in the graph.\n",
        "2.   For each node, apply the following formula: $PR(i) = \\dfrac{(1-d)}{N} + d \\sum_{j \\in In(i)} \\dfrac{PR(j)}{Out(j)}$, where $d$ is the damping factor (usually 0.85) and $In(i)$ is the set of nodes that link to node $i$ and $Out(j)$ is the number of outgoing links from node $j$.\n",
        "3.   Repeat step 2 until convergence.\n",
        "\n",
        "\n",
        "For the practice, you can use the `networkx` library to compute the PageRank scores of the nodes of the graph.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The following cells download the data and parse them to create a dictionary of pages. Each element of the dictionary contains the following information:\n",
        "\n",
        "*   `ID`: the ID of the page\n",
        "*   `quoted_ID`: the ID of the page escaped using HTML encoding\n",
        "*   `categories`: the list of categories of the page\n",
        "*   `out_links`: the list of pages that are linked by the current page\n",
        "\n",
        "Hereafter, an example of the dictionary is shown.\n",
        "\n",
        "```python\n",
        "\n",
        "{\n",
        "    'ID': 'Áedán_mac_Gabráin', \n",
        "    'quoted_ID': '%C3%81ed%C3%A1n_mac_Gabr%C3%A1in', \n",
        "    'categories': ['subject.History.British_History.British_History_1500_and_before_including_Roman_Britain', 'subject.People.Historical_figures'], \n",
        "    'out_links': ['Bede', 'Columba', 'Dál_Riata', 'Great_Britain', 'Ireland', 'Isle_of_Man', 'Monarchy', 'Orkney', 'Picts', 'Scotland', 'Wales']\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "### **Question 1: Pagerank scores**\n",
        "\n",
        "Exploiting the wikipedia citation network, compute, for each page, its associated [pagerank](http://ilpubs.stanford.edu:8090/422/) score.\n",
        "\n",
        "What is the page with the highest Pagerank score?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkbxbh25sZNu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/articles.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/categories.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/links.tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyU76QUUqGI_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install elasticsearch==7.10.1\n",
        "! pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK7a3wt3GvIN"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import unquote\n",
        "\n",
        "list_articles = open(\"articles.tsv\").read()\n",
        "list_articles = list_articles.split(\"\\n\")\n",
        "list_articles = [l for l in list_articles if l!= \"\"]\n",
        "list_articles = [l for l in list_articles if l[0] != \"#\"]\n",
        "unquoted_list_articles = [unquote(l) for l in list_articles if l[0] != \"#\"]\n",
        "dict_articles = {}\n",
        "for i, l in enumerate(unquoted_list_articles):\n",
        "    dict_articles[l] = {}\n",
        "    dict_articles[l][\"ID\"] = l\n",
        "    dict_articles[l][\"quoted_ID\"] = list_articles[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZWrp69VGvF4"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import unquote\n",
        "\n",
        "list_categories = open(\"categories.tsv\").read()\n",
        "list_categories = list_categories.split(\"\\n\")\n",
        "list_categories = [l for l in list_categories if l!= \"\"]\n",
        "list_categories = [l for l in list_categories if l[0] != \"#\"]\n",
        "\n",
        "for l in list_categories:\n",
        "    k, v = l.split(\"\\t\")\n",
        "    k = unquote(k)\n",
        "    v = unquote(v)\n",
        "    if \"categories\" in dict_articles[k].keys():\n",
        "        dict_articles[k][\"categories\"].append(v)\n",
        "    else:\n",
        "        dict_articles[k][\"categories\"] = [v]\n",
        "    \n",
        "print (dict_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZGS5zv5GvDo"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import unquote\n",
        "\n",
        "list_links = open(\"links.tsv\").read()\n",
        "list_links = list_links.split(\"\\n\")\n",
        "list_links = [l for l in list_links if l!= \"\"]\n",
        "list_links = [l for l in list_links if l[0] != \"#\"]\n",
        "\n",
        "for l in list_links:\n",
        "    s, t = l.split(\"\\t\")\n",
        "    s = unquote(s)\n",
        "    t = unquote(t)\n",
        "    if \"out_links\" in dict_articles[s].keys():\n",
        "        dict_articles[s][\"out_links\"].append(t)\n",
        "    else:\n",
        "        dict_articles[s][\"out_links\"] = [t]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yV7vO3aGvA5"
      },
      "outputs": [],
      "source": [
        "print (dict_articles[\"Áedán_mac_Gabráin\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU03LqvbGu-d"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ibyq9Mqn3X"
      },
      "source": [
        "### **Question 2: Wikipedia pages indexing**\n",
        "\n",
        "In this question, you will create an index of the wikipedia pages. It will be used to perform the search of the pages.\n",
        "\n",
        "Create a new index in ElasticSearch and index all the pages (alongiside with their content). Please note that the content of each page can be found at `plaintext_articles/QUOTED_ID_OF_THE_DOC.txt`. For example, the content of the page with ID `Áedán_mac_Gabráin` can be found at `plaintext_articles/%C3%81ed%C3%A1n_mac_Gabr%C3%A1in.txt`. The following cell will download the necessary files.\n",
        "\n",
        "NB: the pagerank score of each page should be stored in the index as a field named `pagerank_score`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8Z9I-eWu1KA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! wget https://github.com/MorenoLaQuatra/DeepNLP/raw/main/practices/P3/plaintext_articles.zip\n",
        "! unzip plaintext_articles.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay4m585sD9Nl"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctgQc008zSB8"
      },
      "source": [
        "### **Question 3: Querying ElasticSearch**\n",
        "\n",
        "After having indexed the wikipedia pages, you can now perform queries to the search engine. You can use the `elasticsearch` library to perform queries. Look for your favorite content (choose and report 3 of them) on the full text of the articles.\n",
        "\n",
        "E.g.:\n",
        "- query 1 : \"The capital of Italy\" (surprised by the result?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "airbSz1qzuAu"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yohg5-V40aUW"
      },
      "source": [
        "### **Question 4: integrating pagerank scores**\n",
        "\n",
        "The standard full-text search engine does not take into account the importance of the pages. In this question, you will modify the query to take into account the pagerank scores of the pages. Create a template query to include pagerank while computing the score (`_score`). \n",
        "\n",
        "Use the [Script score](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-script-score) to generate an hybrid score (`_score + pagerank_score * 250`). 250 is a scaling factor that you can tune to obtain better results. \n",
        "\n",
        "Perform the same set of queries with this modification, does it change the results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3kg790tEZn_"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lQzbdvvelCS"
      },
      "source": [
        "### **Question 5: integrate semantic dense-vectors**\n",
        "\n",
        "Standard full-text search engines are not able to capture the semantic meaning of the words. By default, the search engine will return all the pages that contain the query terms. In this question, you will use the semantic dense-vectors to improve the search engine.\n",
        "\n",
        "**Assignment:** Generate a new index (\"wiki-semantic-search\") including all the information of the previous one plus an additional field that contains a BERT-based embedding vector of the `full_text` of the article. Once indexing is completed, repeat the same queries for a qualitative evaluation of the IR system. \n",
        "\n",
        "**How to create vector embeddings? Some hints:**\n",
        "- Use Sentence-BERT pretrained encoders (www.sbert.net). Choose the most suitable pretrained model (trade off between speed and accuracy). E.g., `multi-qa-MiniLM-L6-cos-v1`\n",
        "- Use cosine similarity to compute the similarity between queries and full text of the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMfjagoifrcY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model_name=\"multi-qa-MiniLM-L6-cos-v1\"\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF3KUfKCC8aV"
      },
      "outputs": [],
      "source": [
        "# create mapping\n",
        "# the following code can be used to create the mapping for the index and instantiate all the properties and the types of the fields\n",
        "\n",
        "dense_dim = len(sentence_encodings[0]) # sentence_encodings is the list of the sentence embeddings\n",
        "\n",
        "index_properties = {}\n",
        "index_properties['settings']={ \"number_of_shards\": 2, \"number_of_replicas\": 1}\n",
        "index_properties['mappings']={ \"dynamic\": \"true\", \"_source\": { \"enabled\": \"true\" }, \"properties\": {}}\n",
        "for t in ['ID', 'quoted_ID', 'full_text']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"text\" }\n",
        "for t in ['pagerank_score']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"float\" }\n",
        "for d in [\"embedding_bert\"]: \n",
        "    index_properties['mappings']['properties'][d]={ \"type\": \"dense_vector\", \"dims\": dense_dim }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke2xcjWNFjIc"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWHgLZhmszNc"
      },
      "source": [
        "## Content-based Recommender Systems\n",
        "\n",
        "A recommender system is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. (source: [Wikipedia](https://en.wikipedia.org/wiki/Recommender_system))\n",
        "\n",
        "In this part of the practice you will be required to generate a text-based unsupervised recommendation system (only **content**-based). The final goal is similar to a IR search engine, the main difference relies on **how you define the \"queries\".**\n",
        "\n",
        "The tools at your disposal are:\n",
        "1. `Sentence-BERT model`: should be used to obtain a vector representation of the input data.\n",
        "2. `ElasticSearch`: can be used for indexing movie information and to perform **fast** similarity search.\n",
        "\n",
        "For the recommendation system you need the following information:\n",
        "- Movie's title\n",
        "- Movie's plot\n",
        "- Plot's embedding vector\n",
        "\n",
        "The dataset used for this goal is: [Wikipedia Movie Plots](https://www.kaggle.com/jrobischon/wikipedia-movie-plots). For this practice you will use a truncated version of the data collection to reduce runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfY5cT58aIk3"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wiki_plots_2005onward.csv\n",
        "import pandas as pd\n",
        "df_movies = pd.read_csv(\"wiki_plots_2005onward.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyP6CMVMa5Gy"
      },
      "source": [
        "### **Question 6: movie encodings**\n",
        "\n",
        "Using the `Sentence-BERT` model, generate a vector representation of the movie plots. Each vector should be stored in the ElasticSearch index to perform similarity search.\n",
        "\n",
        "NB: the vector dimension is dependent on the choice of the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1ldnOtob0LC"
      },
      "outputs": [],
      "source": [
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8zuhSRfFn29"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LODAdsacB9m"
      },
      "source": [
        "### **Question 7: ElasticSearch indexing**\n",
        "\n",
        "Create a new ElasticSearch index (`recsys-movies`) and index all movies with their embedding vectors. In this case, the index should contain the following fields:\n",
        "- `title`: the title of the movie\n",
        "- `plot`: the plot of the movie\n",
        "- `embedding_bert`: the embedding vector of the plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxJkyzioFs_R"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oXh5njnftg2"
      },
      "source": [
        "### **Question 8: Query generation**\n",
        "\n",
        "Create a function that takes as input the following parameters:\n",
        "\n",
        "1. `embedding_model`: Sentence-BERT model used to generate embeddings\n",
        "2. `df_movies`: the dataframe containing all the movies' information\n",
        "3. `movie_title`: a string containing the title of the movie the user is currently watching.\n",
        "\n",
        "It should look for the movie in the dataframe and return the embedding vector of the plot. The `movie_title` is used to retrieve the plot from the dataframe. The `embedding_model` is used to generate the embedding vector of the plot.\n",
        "\n",
        "NB: There must be a 1:1 correspondence between the movie title inserted by the user and the title of the movie in the dataframe. If the title is not found in the dataframe, the function should return an error.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJj22hTTGc_y"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFoTpcZlfkt"
      },
      "source": [
        "### **Question 9: Qualitative evaluation (your personal movie recommendation system)**\n",
        "\n",
        "Evaluate your personal recommendation system by querying for some movies in the data collection. You need to create an elasticsearch query to use the recommendation system (see Q. 5 of this practice).\n",
        "\n",
        "Just some examples:\n",
        "1. title: Harry Potter and the Goblet of Fire\n",
        "2. title: Avengers: Age of Ultron\n",
        "3. title: Star Wars: The Last Jedi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jywjnPb8Gfb-"
      },
      "outputs": [],
      "source": [
        " # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11nD3sRuv7K"
      },
      "source": [
        "### **Question 10 (Bonus)**\n",
        "\n",
        "Rewrite the function at **Q.7** to take multiple movie titles (list of strings). Compute the average embedding vector of the movies and return it as the query vector. It can be used to generate a recommendation system that takes into account multiple movies watched by the user. Perform a qualitative evaluation in this specific case (it is possible to choose movie's titles from the previous list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFWdTm3zHnkA"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPoJDVMPEbd5C9nlL6t5/aX",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Practice 3 -  IR and Recommendation systems.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('asr')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "368d28b5c0b2ca8045e66faa2d9702a27c6ab02973f2f636059018518286f5f3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
