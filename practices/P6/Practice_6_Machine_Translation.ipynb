{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Practice 6 - Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEi+akM7vvByJwE//tGDgv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoLaQuatra/DeepNLP/blob/main/practices/P6/Practice_6_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOEr39Bst_5"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 6:** Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9F11ek-0L8g"
      },
      "source": [
        "## **Machine Translation**\n",
        "\n",
        "It is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n",
        "\n",
        "![](https://www.deepl.com/img/press/desktop_ENIT_2020-01.png)\n",
        "\n",
        "In this practice you will use data collections provided by [tatoeba](https://tatoeba.org/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41mtAy092sCC"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/train_it_en.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/test_it_en.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCTUyoq2rZo"
      },
      "source": [
        "### **Question 1: data reading**\n",
        "\n",
        "Read the data collection and store it into your preferred data structure. It will be used in the subsequent steps. \n",
        "\n",
        "Store train and test set into separate data objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rCk2Jcm4eHF"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyW-AESi3yYe"
      },
      "source": [
        "### **Question 2: pretrained MT models**\n",
        "\n",
        "[EasyNMT](https://github.com/UKPLab/EasyNMT) provides a simple wrapper over HuggingFace transformers library for machine translation. Translate all test sentences from english to italian and viceversa. Store translation in both directions.\n",
        "\n",
        "Note: the choice for the MT model is up to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-f6Huz4bWF"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv-4koeH6yvt"
      },
      "source": [
        "### **Question 3: BLEU scores**\n",
        "\n",
        "Evaluate the selected MT model using [BLEU evaluation metric](https://github.com/mjpost/sacrebleu). Report scores for both translation directions (`EN->IT`, `IT->EN`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9dLMLRw7SDu"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBY0_1X7WAr"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRoH_9ox03ME"
      },
      "source": [
        "### **Question 4: finetuning Seq2Seq model (IT->EN)**\n",
        "\n",
        "Exploit the [Trainer API](https://huggingface.co/transformers/training.html#fine-tuning-in-pytorch-with-the-trainer-api) to finetune and evaluate a [MarianMT](https://arxiv.org/pdf/1804.00344.pdf) sequence to sequence model for machine translation. The documentation for MarianMT is available [here](https://huggingface.co/transformers/model_doc/marian.html).\n",
        "\n",
        "**Note 1:** select the pre-trained model according to the input-output pair (it-en)\n",
        "\n",
        "**Note 2:** for the lab practice, please use a sub-set of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0V98Y1uTrzR"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwS-7GrlSrxF"
      },
      "source": [
        "### **Question 5: Model evaluation**\n",
        "\n",
        "Evaluate the fine-tuned model on the test set provided with the practice. Compute and report the bleu score for the translation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL0kUCsVS7b6"
      },
      "source": [
        "# Your code here)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rig_4YXVXIe"
      },
      "source": [
        "### **Question 6: Seq2Seq model implementation (IT->EN) [BONUS]**\n",
        "\n",
        "Implement a lightweight model for machine translation. It must be trainable on the train set of tatoeba available for the practice.\n",
        "\n",
        "**NOTICE:** the goal is to create **your own network**, not to finetune an existing one. You can also leverage LSTM layers instead of transformers.\n",
        "\n",
        "**Note 1:** The choice of the framework (e.g., Keras, Tensorflow, PyTorch) is up to you.\n",
        "\n",
        "**Note 2:** You must write the architecture and training/evaluation procedures (please do not use out-of-the-box HF models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QPSwu6iTwps"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}