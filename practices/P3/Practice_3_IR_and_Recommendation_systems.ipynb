{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Practice 3 -  IR and Recommendation systems.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoJDVMPEbd5C9nlL6t5/aX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoLaQuatra/DeepNLP/blob/main/practices/P3/Practice_3_IR_and_Recommendation_systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o7x8zVR9gSK"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Information Retrieval & Elastic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIamxNpr_E99"
      },
      "source": [
        "### Download and setup ElasticSearch on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SziAua4o9UIl"
      },
      "source": [
        "# Download and extract elasticsearch\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.10.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN1ll47sQIq-"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# If issues are encountered with this section, ES can be manually started as follows:\n",
        "# ./elasticsearch-7.10.1/bin/elasticsearch\n",
        "\n",
        "# Start and wait for server\n",
        "server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "!sleep 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAAzxrUQFg_g"
      },
      "source": [
        "# wait a bit then test\n",
        "!curl -X GET \"localhost:9200/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bb-wW4sZuw"
      },
      "source": [
        "## Information Retrieval\n",
        "\n",
        "Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of **texts**, images or sounds. (source: Wikipedia).\n",
        "\n",
        "This practice is intended for the creation of a wikipedia-based search engine. For the purpose of the practice, only a subset of the wikipedia pages will be used.\n",
        "\n",
        "Data Source: https://snap.stanford.edu/data/wikispeedia.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XxgpAo-VOg"
      },
      "source": [
        "### **Question 1: Pagerank scores**\n",
        "Exploiting the wikipedia citation network, compute, for each page, its associated [pagerank](http://ilpubs.stanford.edu:8090/422/) score.\n",
        "\n",
        "What is the page with the highest Pagerank score?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkbxbh25sZNu"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/articles.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/categories.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/links.tsv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyU76QUUqGI_"
      },
      "source": [
        "%%capture\n",
        "! pip install elasticsearch==7.10.1\n",
        "! pip install networkx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK7a3wt3GvIN"
      },
      "source": [
        "from urllib.parse import unquote\n",
        "\n",
        "list_articles = open(\"articles.tsv\").read()\n",
        "list_articles = list_articles.split(\"\\n\")\n",
        "list_articles = [l for l in list_articles if l!= \"\"]\n",
        "list_articles = [l for l in list_articles if l[0] != \"#\"]\n",
        "unquoted_list_articles = [unquote(l) for l in list_articles if l[0] != \"#\"]\n",
        "dict_articles = {}\n",
        "for i, l in enumerate(unquoted_list_articles):\n",
        "    dict_articles[l] = {}\n",
        "    dict_articles[l][\"ID\"] = l\n",
        "    dict_articles[l][\"quoted_ID\"] = list_articles[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZWrp69VGvF4"
      },
      "source": [
        "from urllib.parse import unquote\n",
        "\n",
        "list_categories = open(\"categories.tsv\").read()\n",
        "list_categories = list_categories.split(\"\\n\")\n",
        "list_categories = [l for l in list_categories if l!= \"\"]\n",
        "list_categories = [l for l in list_categories if l[0] != \"#\"]\n",
        "\n",
        "for l in list_categories:\n",
        "    k, v = l.split(\"\\t\")\n",
        "    k = unquote(k)\n",
        "    v = unquote(v)\n",
        "    if \"categories\" in dict_articles[k].keys():\n",
        "        dict_articles[k][\"categories\"].append(v)\n",
        "    else:\n",
        "        dict_articles[k][\"categories\"] = [v]\n",
        "    \n",
        "print (dict_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZGS5zv5GvDo"
      },
      "source": [
        "from urllib.parse import unquote\n",
        "\n",
        "list_links = open(\"links.tsv\").read()\n",
        "list_links = list_links.split(\"\\n\")\n",
        "list_links = [l for l in list_links if l!= \"\"]\n",
        "list_links = [l for l in list_links if l[0] != \"#\"]\n",
        "\n",
        "for l in list_links:\n",
        "    s, t = l.split(\"\\t\")\n",
        "    s = unquote(s)\n",
        "    t = unquote(t)\n",
        "    if \"out_links\" in dict_articles[s].keys():\n",
        "        dict_articles[s][\"out_links\"].append(t)\n",
        "    else:\n",
        "        dict_articles[s][\"out_links\"] = [t]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yV7vO3aGvA5"
      },
      "source": [
        "print (dict_articles[\"Áedán_mac_Gabráin\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU03LqvbGu-d"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ibyq9Mqn3X"
      },
      "source": [
        "### **Question 2: Wikipedia pages indexing**\n",
        "\n",
        "Create a new index in ElasticSearch and Index the Wikipedia webpage (alongiside with their content). The content of each page can be found at `plaintext_articles/QUOTED_ID_OF_THE_DOC.txt`\n",
        "\n",
        "NB: pagerank score must be a field of the indexed doc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Z9I-eWu1KA"
      },
      "source": [
        "%%capture\n",
        "! wget https://github.com/MorenoLaQuatra/DeepNLP/raw/main/practices/P3/plaintext_articles.zip\n",
        "! unzip plaintext_articles.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay4m585sD9Nl"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctgQc008zSB8"
      },
      "source": [
        "### **Question 3: Querying ElasticSearch**\n",
        "\n",
        "Perform a query using ElasticSearch. Look for your favorite content (choose and report 3 of them) on the full text of the articles.\n",
        "\n",
        "E.g.:\n",
        "- query 1 : \"The capital of Italy\" (surprised by the result?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "airbSz1qzuAu"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yohg5-V40aUW"
      },
      "source": [
        "### **Question 4: integrating pagerank scores**\n",
        "\n",
        "Create a template query to include pagerank while computing the score (`_score`). \n",
        "\n",
        "Use the [Script score](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-script-score) to generate an hybrid score (`_score + pagerank_score * 250`). \n",
        "\n",
        "Perform the same set of queries with this modification, does it change the results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3kg790tEZn_"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lQzbdvvelCS"
      },
      "source": [
        "### **Question 5: integrate semantic dense-vectors**\n",
        "\n",
        "Generate a new index (\"wiki-semantic-search\") including all the information of the previous one plus an additional field that contains a BERT-based embedding vector of the `full_text` of the article. Once indexing is completed, repeat the same queries for a qualitative evaluation of the IR system. \n",
        "\n",
        "**Some hints below:**\n",
        "- Use Sentence-BERT pretrained encoders (www.sbert.net). Choose the most suitable pretrained model (trade off between speed and accuracy). E.g., `multi-qa-MiniLM-L6-cos-v1`\n",
        "- Use cosine similarity to compute the similarity between queries and full text of the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMfjagoifrcY"
      },
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF3KUfKCC8aV"
      },
      "source": [
        "# create mapping\n",
        "\n",
        "dense_dim = len(sentence_encodings[0])\n",
        "\n",
        "index_properties = {}\n",
        "index_properties['settings']={ \"number_of_shards\": 2, \"number_of_replicas\": 1}\n",
        "index_properties['mappings']={ \"dynamic\": \"true\", \"_source\": { \"enabled\": \"true\" }, \"properties\": {}}\n",
        "for t in ['ID', 'quoted_ID', 'full_text']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"text\" }\n",
        "for t in ['pagerank_score']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"float\" }\n",
        "for d in [\"embedding_bert\"]: \n",
        "    index_properties['mappings']['properties'][d]={ \"type\": \"dense_vector\", \"dims\": dense_dim }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke2xcjWNFjIc"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWHgLZhmszNc"
      },
      "source": [
        "## Content-based Recommender Systems\n",
        "\n",
        "A recommender system is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. (source: [Wikipedia](https://en.wikipedia.org/wiki/Recommender_system))\n",
        "\n",
        "In this part of the practice you will be required to generate a text-based unsupervised recommendation system (only **content**-based). The final goal is similar to a IR search engine, the main difference relies on **how you define the \"queries\".**\n",
        "\n",
        "The tools at your disposal are:\n",
        "1. `Sentence-BERT model`: should be used to obtain a vector representation of the input data.\n",
        "2. `ElasticSearch`: can be used for indexing movie information and to perform **fast** similarity search.\n",
        "\n",
        "For the recommendation system you need the following information:\n",
        "- Movie's title\n",
        "- Movie's plot\n",
        "- Plot's embedding vector\n",
        "\n",
        "The dataset used for this goal is: [Wikipedia Movie Plots](https://www.kaggle.com/jrobischon/wikipedia-movie-plots). For this practice you will use a truncated version of the data collection to reduce runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfY5cT58aIk3"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wiki_plots_2005onward.csv\n",
        "import pandas as pd\n",
        "df_movies = pd.read_csv(\"wiki_plots_2005onward.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyP6CMVMa5Gy"
      },
      "source": [
        "### **Question 6: movie encodings**\n",
        "\n",
        "Use Sentence-BERT model to encode movie plots into fixed-size vectors.\n",
        "\n",
        "NB: the vector dimension is dependent on the choice of the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1ldnOtob0LC"
      },
      "source": [
        "! pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8zuhSRfFn29"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LODAdsacB9m"
      },
      "source": [
        "### **Question 7: ElasticSearch indexing**\n",
        "\n",
        "Create a new ElasticSearch index (`recsys-movies`) and index all movies with their embedding vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxJkyzioFs_R"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oXh5njnftg2"
      },
      "source": [
        "### **Question 8: Query generation**\n",
        "\n",
        "Create a function that accept the following arguments:\n",
        "1. `embedding_model`: Sentence-BERT model used to generate embeddings\n",
        "2. `df_movies`: the dataframe containing all the movies' information\n",
        "3. `movie_title`: a string containing the title of the movie the user is currently watching.\n",
        "\n",
        "It should return the embedding vector associated to the query by looking for the `movie_title` plot in `df_movies`. It uses `embedding_model` to encode it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJj22hTTGc_y"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFoTpcZlfkt"
      },
      "source": [
        "### **Question 8: Qualitative evaluation (your personal movie recommendation system)**\n",
        "\n",
        "Evaluate your personal recommendation system by querying for some movies in the data collection. You need to create an elasticsearch query to use the recommendation system (see Q. 5 of this practice).\n",
        "\n",
        "Just some examples:\n",
        "1. title: Harry Potter and the Goblet of Fire\n",
        "2. title: Avengers: Age of Ultron\n",
        "3. title: Star Wars: The Last Jedi\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jywjnPb8Gfb-"
      },
      "source": [
        " # Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11nD3sRuv7K"
      },
      "source": [
        "### **Question 9 (Bonus)**\n",
        "\n",
        "Rewrite the function at Q.7 to take multiple movie titles (list of strings). Compute the average vector and use it to obtain recommendations. Perform a qualitative evaluation in this specific case (it is possible to choose movie's titles from the previous list)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFWdTm3zHnkA"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}